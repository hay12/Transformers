# Transformer Architecture: Attention Is All You Need

## Introduction

This repository contains a PyTorch implementation of the Transformer architecture, as proposed in the paper "Attention Is All You Need" by Vaswani et al. The original paper can be accessed [here](https://arxiv.org/pdf/1706.03762.pdf).

- This Git repository will be followed by an article that will break each part of it and explain it fully.
  - The Hebrew Version article can be found [here](https://docs.google.com/document/d/1ySJddUs4bFewUiJ2J4tT_0I_FxP59vA8eAXEJrHV2eo/edit?usp=sharing)
  - The English Version article can be found [here](https://docs.google.com/document/d/1aggiNVzVek-evQ1HiVyY8OTLg0H3NX4oX7WBVbCNumo/edit?usp=sharing)
- We also wrote a full paper analysis both in Hebrew and English that can be found here:
  - [Hebrew Version](https://docs.google.com/document/d/1xBkt3H7ffNtcEh__Cnw9Mw3dVgjAN8je0kOy_m5WXyA/edit?usp=sharing)
  - [English Version](https://docs.google.com/document/d/19gLgIY17IOnaATim8inncxGW4LV_4y9AeVdMfY1dPGk/edit?usp=sharing)

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)

## Features

- Encoder-Decoder Architecture
- Multi-Head Attention Mechanism
- Positional Encoding
- Layer Normalization

## Installation

```bash
git clone https://github.com/your_username/transformer-architecture.git
cd transformer-architecture
pip install -r requirements.txt
```



